{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import sine_data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Change  self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) to self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_l1=0, weight_regularizer_l2=0,bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "       # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    \n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "            \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) +self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) +self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        \n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "            \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -(1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dvalues, y_true):\n",
    "    # Number of samples\n",
    "    samples = len(dvalues)\n",
    "    # Number of labels in every sample\n",
    "    # We'll use the first sample to count them\n",
    "    labels = len(dvalues[0])\n",
    "    # If labels are sparse, turn them into one-hot vector\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "    # Calculate gradient\n",
    "    self.dinputs = -y_true / dvalues\n",
    "    # Normalize gradient\n",
    "    self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    # Creates activation and loss function objects\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pas\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100, acc: 0.005, loss: 0.132 (data_loss: 0.132, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 200, acc: 0.005, loss: 0.100 (data_loss: 0.100, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 300, acc: 0.006, loss: 0.068 (data_loss: 0.068, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 400, acc: 0.049, loss: 0.028 (data_loss: 0.028, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 500, acc: 0.095, loss: 0.005 (data_loss: 0.005, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 600, acc: 0.401, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 700, acc: 0.535, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 800, acc: 0.592, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 900, acc: 0.620, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1000, acc: 0.650, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1100, acc: 0.675, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1200, acc: 0.699, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1300, acc: 0.690, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1400, acc: 0.697, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1500, acc: 0.721, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1600, acc: 0.688, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1700, acc: 0.753, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1800, acc: 0.719, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1900, acc: 0.764, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2000, acc: 0.703, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2100, acc: 0.767, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2200, acc: 0.699, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2300, acc: 0.770, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2400, acc: 0.774, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2500, acc: 0.723, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2600, acc: 0.777, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2700, acc: 0.698, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2800, acc: 0.781, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2900, acc: 0.782, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3000, acc: 0.787, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3100, acc: 0.783, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3200, acc: 0.785, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3300, acc: 0.785, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3400, acc: 0.786, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3500, acc: 0.204, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3600, acc: 0.789, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3700, acc: 0.791, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3800, acc: 0.793, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3900, acc: 0.794, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4000, acc: 0.795, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4100, acc: 0.791, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4200, acc: 0.795, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4300, acc: 0.793, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4400, acc: 0.800, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4500, acc: 0.799, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4600, acc: 0.801, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4700, acc: 0.800, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4800, acc: 0.234, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4900, acc: 0.803, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5000, acc: 0.801, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5100, acc: 0.805, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5200, acc: 0.807, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5300, acc: 0.803, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5400, acc: 0.809, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5500, acc: 0.766, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5600, acc: 0.814, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5700, acc: 0.825, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5800, acc: 0.815, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5900, acc: 0.830, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6000, acc: 0.814, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6100, acc: 0.826, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6200, acc: 0.821, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6300, acc: 0.819, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6400, acc: 0.825, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6500, acc: 0.822, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6600, acc: 0.834, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6700, acc: 0.825, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6800, acc: 0.555, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6900, acc: 0.829, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7000, acc: 0.841, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7100, acc: 0.825, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7200, acc: 0.827, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7300, acc: 0.808, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7400, acc: 0.829, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7500, acc: 0.057, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7600, acc: 0.830, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7700, acc: 0.794, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7800, acc: 0.828, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7900, acc: 0.840, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8000, acc: 0.839, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8100, acc: 0.841, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8200, acc: 0.176, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8300, acc: 0.842, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8400, acc: 0.837, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8500, acc: 0.843, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8600, acc: 0.841, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8700, acc: 0.847, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8800, acc: 0.843, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8900, acc: 0.811, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9000, acc: 0.844, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9100, acc: 0.842, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9200, acc: 0.844, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9300, acc: 0.059, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9400, acc: 0.847, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9500, acc: 0.859, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9600, acc: 0.848, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9700, acc: 0.849, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9800, acc: 0.845, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9900, acc: 0.849, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 10000, acc: 0.838, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# Create Dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Create third Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam()\n",
    "\n",
    "# Accuracy precision for accuracy calculation\n",
    "# There are no really accuracy factor for regression problem,\n",
    "# but we can simulate/approximate it. We'll calculate it by checking\n",
    "# how many values have a difference to their ground truth equivalent\n",
    "# less than given precision\n",
    "# We'll calculate this precision as a fraction of standard deviation\n",
    "# of al the ground truth values\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "loss_acum = []\n",
    "accuracy_acum = []\n",
    "count = []\n",
    "y_hat = []\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through third Dense layer\n",
    "    # takes outputs of activation function of second layer as inputs\n",
    "    dense3.forward(activation2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of third dense layer here\n",
    "    activation3.forward(dense3.output)\n",
    "    \n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "    \n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)+ loss_function.regularization_loss(dense3)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # To calculate it we're taking absolute difference between\n",
    "    # predictions and ground truth values and compare if differences\n",
    "    # are lower than given precision value\n",
    "    \n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    loss_acum.append(loss)\n",
    "    accuracy_acum.append(accuracy)\n",
    "    y_hat = predictions\n",
    "    count.append(epoch)\n",
    "        \n",
    "    \n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(y=accuracy_acum , x=count)\n",
    "sns.lineplot(y=loss_acum, x=count)\n",
    "# plt.ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import sine_data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_l1=0, weight_regularizer_l2=0,bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "       # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    \n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "            \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) +self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) +self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        \n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "            \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -(1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dvalues, y_true):\n",
    "    # Number of samples\n",
    "    samples = len(dvalues)\n",
    "    # Number of labels in every sample\n",
    "    # We'll use the first sample to count them\n",
    "    labels = len(dvalues[0])\n",
    "    # If labels are sparse, turn them into one-hot vector\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "    # Calculate gradient\n",
    "    self.dinputs = -y_true / dvalues\n",
    "    # Normalize gradient\n",
    "    self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    # Creates activation and loss function objects\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pas\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.500 (data_loss: 0.500, reg_loss: 0.000), lr: 0.005\n",
      "epoch: 100, acc: 0.007, loss: 0.084 (data_loss: 0.084, reg_loss: 0.000), lr: 0.004549590536851684\n",
      "epoch: 200, acc: 0.033, loss: 0.034 (data_loss: 0.034, reg_loss: 0.000), lr: 0.004170141784820684\n",
      "epoch: 300, acc: 0.020, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.003849114703618168\n",
      "epoch: 400, acc: 0.618, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0035739814152966403\n",
      "epoch: 500, acc: 0.605, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00333555703802535\n",
      "epoch: 600, acc: 0.732, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0031269543464665416\n",
      "epoch: 700, acc: 0.763, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002942907592701589\n",
      "epoch: 800, acc: 0.778, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0027793218454697055\n",
      "epoch: 900, acc: 0.789, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0026329647182727752\n",
      "epoch: 1000, acc: 0.135, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002501250625312656\n",
      "epoch: 1100, acc: 0.825, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0023820867079561697\n",
      "epoch: 1200, acc: 0.837, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002273760800363802\n",
      "epoch: 1300, acc: 0.453, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002174858634188778\n",
      "epoch: 1400, acc: 0.853, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020842017507294707\n",
      "epoch: 1500, acc: 0.857, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020008003201280513\n",
      "epoch: 1600, acc: 0.861, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001923816852635629\n",
      "epoch: 1700, acc: 0.869, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001852537977028529\n",
      "epoch: 1800, acc: 0.874, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017863522686673815\n",
      "epoch: 1900, acc: 0.882, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017247326664367024\n",
      "epoch: 2000, acc: 0.885, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016672224074691564\n",
      "epoch: 2100, acc: 0.887, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016134236850596968\n",
      "epoch: 2200, acc: 0.891, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015629884338855893\n",
      "epoch: 2300, acc: 0.896, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015156107911488332\n",
      "epoch: 2400, acc: 0.899, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014710208884966167\n",
      "epoch: 2500, acc: 0.898, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014289797084881396\n",
      "epoch: 2600, acc: 0.906, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001389274798555154\n",
      "epoch: 2700, acc: 0.908, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013517166801838335\n",
      "epoch: 2800, acc: 0.919, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013161358252171624\n",
      "epoch: 2900, acc: 0.920, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012823800974608873\n",
      "epoch: 3000, acc: 0.156, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012503125781445363\n",
      "epoch: 3100, acc: 0.929, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012198097096852891\n",
      "epoch: 3200, acc: 0.927, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011907597046915933\n",
      "epoch: 3300, acc: 0.190, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011630611770179114\n",
      "epoch: 3400, acc: 0.927, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011366219595362584\n",
      "epoch: 3500, acc: 0.928, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011113580795732384\n",
      "epoch: 3600, acc: 0.915, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010871928680147858\n",
      "epoch: 3700, acc: 0.935, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010640561821664183\n",
      "epoch: 3800, acc: 0.933, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010418837257762034\n",
      "epoch: 3900, acc: 0.242, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010206164523372118\n",
      "epoch: 4000, acc: 0.935, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010002000400080014\n",
      "epoch: 4100, acc: 0.934, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009805844283192783\n",
      "epoch: 4200, acc: 0.748, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009617234083477593\n",
      "epoch: 4300, acc: 0.935, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009435742592942063\n",
      "epoch: 4400, acc: 0.936, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009260974254491572\n",
      "epoch: 4500, acc: 0.930, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009092562284051646\n",
      "epoch: 4600, acc: 0.937, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000893016610108948\n",
      "epoch: 4700, acc: 0.463, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008773469029654326\n",
      "epoch: 4800, acc: 0.941, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000862217623728229\n",
      "epoch: 4900, acc: 0.943, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008476012883539582\n",
      "epoch: 5000, acc: 0.877, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008334722453742291\n",
      "epoch: 5100, acc: 0.944, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008198065256599442\n",
      "epoch: 5200, acc: 0.944, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008065817067268914\n",
      "epoch: 5300, acc: 0.463, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007937767899666614\n",
      "epoch: 5400, acc: 0.943, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007813720893889669\n",
      "epoch: 5500, acc: 0.945, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007693491306354824\n",
      "epoch: 5600, acc: 0.155, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007576905591756327\n",
      "epoch: 5700, acc: 0.948, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007463800567248844\n",
      "epoch: 5800, acc: 0.949, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007354022650389764\n",
      "epoch: 5900, acc: 0.950, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007247427163357008\n",
      "epoch: 6000, acc: 0.950, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000714387769681383\n",
      "epoch: 6100, acc: 0.950, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007043245527539089\n",
      "epoch: 6200, acc: 0.901, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006945409084595084\n",
      "epoch: 6300, acc: 0.952, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006850253459377996\n",
      "epoch: 6400, acc: 0.951, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006757669955399379\n",
      "epoch: 6500, acc: 0.953, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006667555674089878\n",
      "epoch: 6600, acc: 0.957, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006579813133307014\n",
      "epoch: 6700, acc: 0.953, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006494349915573451\n",
      "epoch: 6800, acc: 0.955, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006411078343377356\n",
      "epoch: 6900, acc: 0.958, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00063299151791366\n",
      "epoch: 7000, acc: 0.957, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006250781347668457\n",
      "epoch: 7100, acc: 0.958, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006173601679219657\n",
      "epoch: 7200, acc: 0.744, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006098304671301379\n",
      "epoch: 7300, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006024822267743102\n",
      "epoch: 7400, acc: 0.957, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005953089653530181\n",
      "epoch: 7500, acc: 0.954, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000588304506412519\n",
      "epoch: 7600, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005814629608093965\n",
      "epoch: 7700, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005747787101965744\n",
      "epoch: 7800, acc: 0.960, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005682463916354131\n",
      "epoch: 7900, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005618608832453085\n",
      "epoch: 8000, acc: 0.961, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00055561729081009\n",
      "epoch: 8100, acc: 0.867, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005495109352676119\n",
      "epoch: 8200, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005435373410153278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8300, acc: 0.955, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005376922249704269\n",
      "epoch: 8400, acc: 0.968, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005319714863283328\n",
      "epoch: 8500, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005263711969681019\n",
      "epoch: 8600, acc: 0.966, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005208875924575476\n",
      "epoch: 8700, acc: 0.967, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005155170636148056\n",
      "epoch: 8800, acc: 0.972, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005102561485865905\n",
      "epoch: 8900, acc: 0.969, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005051015254066068\n",
      "epoch: 9000, acc: 0.967, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005000500050005\n",
      "epoch: 9100, acc: 0.968, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004950985246063966\n",
      "epoch: 9200, acc: 0.959, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004902441415825081\n",
      "epoch: 9300, acc: 0.971, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004854840275754928\n",
      "epoch: 9400, acc: 0.968, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004808154630252909\n",
      "epoch: 9500, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047623583198399844\n",
      "epoch: 9600, acc: 0.969, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047174261722804036\n",
      "epoch: 9700, acc: 0.969, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046733339564445275\n",
      "epoch: 9800, acc: 0.972, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046300583387350687\n",
      "epoch: 9900, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045875768419121016\n",
      "epoch: 10000, acc: 0.970, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045458678061641964\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# Create Dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Create third Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
    "\n",
    "# Accuracy precision for accuracy calculation\n",
    "# There are no really accuracy factor for regression problem,\n",
    "# but we can simulate/approximate it. We'll calculate it by checking\n",
    "# how many values have a difference to their ground truth equivalent\n",
    "# less than given precision\n",
    "# We'll calculate this precision as a fraction of standard deviation\n",
    "# of al the ground truth values\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "loss_acum = []\n",
    "accuracy_acum = []\n",
    "count = []\n",
    "y_hat = []\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through third Dense layer\n",
    "    # takes outputs of activation function of second layer as inputs\n",
    "    dense3.forward(activation2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of third dense layer here\n",
    "    activation3.forward(dense3.output)\n",
    "    \n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "    \n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)+ loss_function.regularization_loss(dense3)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # To calculate it we're taking absolute difference between\n",
    "    # predictions and ground truth values and compare if differences\n",
    "    # are lower than given precision value\n",
    "    \n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    loss_acum.append(loss)\n",
    "    accuracy_acum.append(accuracy)\n",
    "    y_hat = predictions\n",
    "    count.append(epoch)\n",
    "        \n",
    "    \n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(y=accuracy_acum , x=count)\n",
    "sns.lineplot(y=loss_acum, x=count)\n",
    "# plt.ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import sine_data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_l1=0, weight_regularizer_l2=0,bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "       # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    \n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "            \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) +self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) +self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        \n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "            \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -(1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dvalues, y_true):\n",
    "    # Number of samples\n",
    "    samples = len(dvalues)\n",
    "    # Number of labels in every sample\n",
    "    # We'll use the first sample to count them\n",
    "    labels = len(dvalues[0])\n",
    "    # If labels are sparse, turn them into one-hot vector\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "    # Calculate gradient\n",
    "    self.dinputs = -y_true / dvalues\n",
    "    # Normalize gradient\n",
    "    self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    # Creates activation and loss function objects\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pas\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.500 (data_loss: 0.500, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100, acc: 0.003, loss: 0.187 (data_loss: 0.187, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 200, acc: 0.006, loss: 0.108 (data_loss: 0.108, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 300, acc: 0.005, loss: 0.092 (data_loss: 0.092, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 400, acc: 0.007, loss: 0.079 (data_loss: 0.079, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 500, acc: 0.010, loss: 0.071 (data_loss: 0.071, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 600, acc: 0.014, loss: 0.065 (data_loss: 0.065, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 700, acc: 0.092, loss: 0.059 (data_loss: 0.059, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 800, acc: 0.082, loss: 0.051 (data_loss: 0.051, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 900, acc: 0.075, loss: 0.043 (data_loss: 0.043, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1000, acc: 0.108, loss: 0.035 (data_loss: 0.035, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1100, acc: 0.163, loss: 0.032 (data_loss: 0.032, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1200, acc: 0.208, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1300, acc: 0.295, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1400, acc: 0.371, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1500, acc: 0.426, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1600, acc: 0.468, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1700, acc: 0.443, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1800, acc: 0.494, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1900, acc: 0.513, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2000, acc: 0.331, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2100, acc: 0.527, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2200, acc: 0.540, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2300, acc: 0.544, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2400, acc: 0.500, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2500, acc: 0.555, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2600, acc: 0.560, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2700, acc: 0.566, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2800, acc: 0.571, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2900, acc: 0.578, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3000, acc: 0.580, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3100, acc: 0.589, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3200, acc: 0.509, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3300, acc: 0.602, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3400, acc: 0.604, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3500, acc: 0.607, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3600, acc: 0.607, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3700, acc: 0.569, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3800, acc: 0.609, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3900, acc: 0.511, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4000, acc: 0.611, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4100, acc: 0.284, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4200, acc: 0.613, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4300, acc: 0.573, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4400, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4500, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4600, acc: 0.576, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4700, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4800, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4900, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5000, acc: 0.614, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5100, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5200, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5300, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5400, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5500, acc: 0.614, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5600, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5700, acc: 0.229, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5800, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5900, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6000, acc: 0.440, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6100, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6200, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6300, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6400, acc: 0.607, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6500, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6600, acc: 0.360, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6700, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6800, acc: 0.436, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6900, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7000, acc: 0.616, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7100, acc: 0.618, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7200, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7300, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7400, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7500, acc: 0.116, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7600, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7700, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7800, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7900, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8000, acc: 0.510, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8100, acc: 0.618, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8200, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8300, acc: 0.618, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8400, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8500, acc: 0.618, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8600, acc: 0.618, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8700, acc: 0.621, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8800, acc: 0.618, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8900, acc: 0.624, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9000, acc: 0.619, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9100, acc: 0.551, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9200, acc: 0.620, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9300, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9400, acc: 0.599, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9500, acc: 0.620, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9600, acc: 0.615, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9700, acc: 0.619, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9800, acc: 0.620, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9900, acc: 0.617, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 10000, acc: 0.620, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# Create Dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Create third Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam()\n",
    "\n",
    "# Accuracy precision for accuracy calculation\n",
    "# There are no really accuracy factor for regression problem,\n",
    "# but we can simulate/approximate it. We'll calculate it by checking\n",
    "# how many values have a difference to their ground truth equivalent\n",
    "# less than given precision\n",
    "# We'll calculate this precision as a fraction of standard deviation\n",
    "# of al the ground truth values\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "loss_acum = []\n",
    "accuracy_acum = []\n",
    "count = []\n",
    "y_hat = []\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through third Dense layer\n",
    "    # takes outputs of activation function of second layer as inputs\n",
    "    dense3.forward(activation2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of third dense layer here\n",
    "    activation3.forward(dense3.output)\n",
    "    \n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "    \n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)+ loss_function.regularization_loss(dense3)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # To calculate it we're taking absolute difference between\n",
    "    # predictions and ground truth values and compare if differences\n",
    "    # are lower than given precision value\n",
    "    \n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    loss_acum.append(loss)\n",
    "    accuracy_acum.append(accuracy)\n",
    "    y_hat = predictions\n",
    "    count.append(epoch)\n",
    "        \n",
    "    \n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(y=accuracy_acum , x=count)\n",
    "sns.lineplot(y=loss_acum, x=count)\n",
    "# plt.ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

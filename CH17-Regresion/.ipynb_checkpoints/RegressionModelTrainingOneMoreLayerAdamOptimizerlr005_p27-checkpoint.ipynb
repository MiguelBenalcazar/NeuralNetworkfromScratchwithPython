{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import sine_data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_l1=0, weight_regularizer_l2=0,bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "    \n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "       # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    \n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "            \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) +self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) +self.epsilon)\n",
    "        \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        \n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "            \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -(1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dvalues, y_true):\n",
    "    # Number of samples\n",
    "    samples = len(dvalues)\n",
    "    # Number of labels in every sample\n",
    "    # We'll use the first sample to count them\n",
    "    labels = len(dvalues[0])\n",
    "    # If labels are sparse, turn them into one-hot vector\n",
    "    if len(y_true.shape) == 1:\n",
    "        y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "    # Calculate gradient\n",
    "    self.dinputs = -y_true / dvalues\n",
    "    # Normalize gradient\n",
    "    self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    # Creates activation and loss function objects\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pas\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.500 (data_loss: 0.500, reg_loss: 0.000), lr: 0.05\n",
      "epoch: 100, acc: 0.087, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.04549590536851684\n",
      "epoch: 200, acc: 0.009, loss: 0.032 (data_loss: 0.032, reg_loss: 0.000), lr: 0.041701417848206836\n",
      "epoch: 300, acc: 0.103, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.03849114703618168\n",
      "epoch: 400, acc: 0.107, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.035739814152966405\n",
      "epoch: 500, acc: 0.076, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0333555703802535\n",
      "epoch: 600, acc: 0.071, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.03126954346466542\n",
      "epoch: 700, acc: 0.015, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.029429075927015894\n",
      "epoch: 800, acc: 0.032, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.027793218454697056\n",
      "epoch: 900, acc: 0.015, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.02632964718272775\n",
      "epoch: 1000, acc: 0.111, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.02501250625312656\n",
      "epoch: 1100, acc: 0.105, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.023820867079561697\n",
      "epoch: 1200, acc: 0.108, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.02273760800363802\n",
      "epoch: 1300, acc: 0.111, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.02174858634188778\n",
      "epoch: 1400, acc: 0.077, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.020842017507294707\n",
      "epoch: 1500, acc: 0.024, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.020008003201280513\n",
      "epoch: 1600, acc: 0.098, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.019238168526356292\n",
      "epoch: 1700, acc: 0.110, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.018525379770285292\n",
      "epoch: 1800, acc: 0.114, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.017863522686673815\n",
      "epoch: 1900, acc: 0.113, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.017247326664367024\n",
      "epoch: 2000, acc: 0.066, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.016672224074691565\n",
      "epoch: 2100, acc: 0.078, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.016134236850596968\n",
      "epoch: 2200, acc: 0.096, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.015629884338855895\n",
      "epoch: 2300, acc: 0.092, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.015156107911488331\n",
      "epoch: 2400, acc: 0.016, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.014710208884966167\n",
      "epoch: 2500, acc: 0.123, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.014289797084881395\n",
      "epoch: 2600, acc: 0.120, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.01389274798555154\n",
      "epoch: 2700, acc: 0.021, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.013517166801838336\n",
      "epoch: 2800, acc: 0.125, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.013161358252171624\n",
      "epoch: 2900, acc: 0.129, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.012823800974608874\n",
      "epoch: 3000, acc: 0.107, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.012503125781445364\n",
      "epoch: 3100, acc: 0.089, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.012198097096852892\n",
      "epoch: 3200, acc: 0.098, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.011907597046915934\n",
      "epoch: 3300, acc: 0.146, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.011630611770179114\n",
      "epoch: 3400, acc: 0.131, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.011366219595362582\n",
      "epoch: 3500, acc: 0.132, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.011113580795732384\n",
      "epoch: 3600, acc: 0.136, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.010871928680147858\n",
      "epoch: 3700, acc: 0.137, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.010640561821664184\n",
      "epoch: 3800, acc: 0.139, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.010418837257762034\n",
      "epoch: 3900, acc: 0.030, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.010206164523372118\n",
      "epoch: 4000, acc: 0.072, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.010002000400080015\n",
      "epoch: 4100, acc: 0.030, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.009805844283192783\n",
      "epoch: 4200, acc: 0.035, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.009617234083477593\n",
      "epoch: 4300, acc: 0.099, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.009435742592942064\n",
      "epoch: 4400, acc: 0.037, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.009260974254491572\n",
      "epoch: 4500, acc: 0.212, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.009092562284051647\n",
      "epoch: 4600, acc: 0.170, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.00893016610108948\n",
      "epoch: 4700, acc: 0.167, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.008773469029654325\n",
      "epoch: 4800, acc: 0.175, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.00862217623728229\n",
      "epoch: 4900, acc: 0.173, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.008476012883539583\n",
      "epoch: 5000, acc: 0.023, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.008334722453742291\n",
      "epoch: 5100, acc: 0.188, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.008198065256599442\n",
      "epoch: 5200, acc: 0.193, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.008065817067268914\n",
      "epoch: 5300, acc: 0.189, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.007937767899666614\n",
      "epoch: 5400, acc: 0.202, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.00781372089388967\n",
      "epoch: 5500, acc: 0.225, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.007693491306354824\n",
      "epoch: 5600, acc: 0.203, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.007576905591756326\n",
      "epoch: 5700, acc: 0.236, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.007463800567248844\n",
      "epoch: 5800, acc: 0.105, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.007354022650389764\n",
      "epoch: 5900, acc: 0.242, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.007247427163357008\n",
      "epoch: 6000, acc: 0.212, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0071438776968138305\n",
      "epoch: 6100, acc: 0.218, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.00704324552753909\n",
      "epoch: 6200, acc: 0.092, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006945409084595084\n",
      "epoch: 6300, acc: 0.225, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006850253459377996\n",
      "epoch: 6400, acc: 0.214, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0067576699553993785\n",
      "epoch: 6500, acc: 0.023, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006667555674089879\n",
      "epoch: 6600, acc: 0.217, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006579813133307014\n",
      "epoch: 6700, acc: 0.084, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006494349915573451\n",
      "epoch: 6800, acc: 0.044, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006411078343377357\n",
      "epoch: 6900, acc: 0.074, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0063299151791366\n",
      "epoch: 7000, acc: 0.036, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006250781347668458\n",
      "epoch: 7100, acc: 0.017, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006173601679219657\n",
      "epoch: 7200, acc: 0.235, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006098304671301379\n",
      "epoch: 7300, acc: 0.249, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.006024822267743102\n",
      "epoch: 7400, acc: 0.224, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005953089653530182\n",
      "epoch: 7500, acc: 0.137, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005883045064125191\n",
      "epoch: 7600, acc: 0.241, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005814629608093965\n",
      "epoch: 7700, acc: 0.244, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005747787101965744\n",
      "epoch: 7800, acc: 0.069, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005682463916354132\n",
      "epoch: 7900, acc: 0.270, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0056186088324530845\n",
      "epoch: 8000, acc: 0.259, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0055561729081009\n",
      "epoch: 8100, acc: 0.239, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005495109352676119\n",
      "epoch: 8200, acc: 0.259, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005435373410153278\n",
      "epoch: 8300, acc: 0.051, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.00537692224970427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8400, acc: 0.038, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005319714863283327\n",
      "epoch: 8500, acc: 0.240, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005263711969681019\n",
      "epoch: 8600, acc: 0.259, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005208875924575477\n",
      "epoch: 8700, acc: 0.235, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005155170636148057\n",
      "epoch: 8800, acc: 0.106, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005102561485865905\n",
      "epoch: 8900, acc: 0.271, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0050510152540660675\n",
      "epoch: 9000, acc: 0.022, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.005000500050005001\n",
      "epoch: 9100, acc: 0.242, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004950985246063967\n",
      "epoch: 9200, acc: 0.240, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004902441415825081\n",
      "epoch: 9300, acc: 0.231, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0048548402757549285\n",
      "epoch: 9400, acc: 0.133, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004808154630252909\n",
      "epoch: 9500, acc: 0.244, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004762358319839985\n",
      "epoch: 9600, acc: 0.248, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004717426172280404\n",
      "epoch: 9700, acc: 0.239, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004673333956444528\n",
      "epoch: 9800, acc: 0.270, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004630058338735069\n",
      "epoch: 9900, acc: 0.275, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.004587576841912101\n",
      "epoch: 10000, acc: 0.229, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.0045458678061641965\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# Create Dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Create third Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=1e-3)\n",
    "\n",
    "# Accuracy precision for accuracy calculation\n",
    "# There are no really accuracy factor for regression problem,\n",
    "# but we can simulate/approximate it. We'll calculate it by checking\n",
    "# how many values have a difference to their ground truth equivalent\n",
    "# less than given precision\n",
    "# We'll calculate this precision as a fraction of standard deviation\n",
    "# of al the ground truth values\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "loss_acum = []\n",
    "accuracy_acum = []\n",
    "count = []\n",
    "y_hat = []\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through third Dense layer\n",
    "    # takes outputs of activation function of second layer as inputs\n",
    "    dense3.forward(activation2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of third dense layer here\n",
    "    activation3.forward(dense3.output)\n",
    "    \n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "    \n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)+ loss_function.regularization_loss(dense3)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # To calculate it we're taking absolute difference between\n",
    "    # predictions and ground truth values and compare if differences\n",
    "    # are lower than given precision value\n",
    "    \n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    loss_acum.append(loss)\n",
    "    accuracy_acum.append(accuracy)\n",
    "    y_hat = predictions\n",
    "    count.append(epoch)\n",
    "        \n",
    "    \n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(y=accuracy_acum , x=count)\n",
    "sns.lineplot(y=loss_acum, x=count)\n",
    "# plt.ylim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lambda_l1w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b25acb92655c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml1w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_l1w\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ml1b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_l1b\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ml2w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_l2w\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ml2b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_l2b\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml1w\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml1b\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml2w\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml2b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lambda_l1w' is not defined"
     ]
    }
   ],
   "source": [
    "l1w = lambda_l1w * sum(abs(weights))\n",
    "l1b = lambda_l1b * sum(abs(biases))\n",
    "l2w = lambda_l2w * sum(weights**2)\n",
    "l2b = lambda_l2b * sum(biases**2)\n",
    "loss = data_loss + l1w + l1b + l2w + l2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer initialization\n",
    "def __init__(self, n_inputs, n_neurons,weight_regularizer_l1=0, weight_regularizer_l2=0,bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "    self.biases = np.zeros((1, neurons))\n",
    "    \n",
    "    # Set regularization strength\n",
    "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "    self.bias_regularizer_l2 = bias_regularizer_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization loss calculation\n",
    "def regularization_loss(self, layer):\n",
    "    # 0 by default\n",
    "    regularization_loss = 0\n",
    "    \n",
    "    # L1 regularization - weights\n",
    "    # calculate only when factor greater than 0\n",
    "    if layer.weight_regularizer_l1 > 0:\n",
    "        regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))# L2 regularization - weights\n",
    "        \n",
    "    if layer.weight_regularizer_l2 > 0:\n",
    "        regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "    # L1 regularization - biases\n",
    "    # calculate only when factor greater than 0\n",
    "    if layer.bias_regularizer_l1 > 0:\n",
    "        regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "    \n",
    "    # L2 regularization - biases\n",
    "    if layer.bias_regularizer_l2 > 0:\n",
    "        regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "    return regularization_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-167bad85c651>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Calculate loss from output of activation2 so softmax activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Calculate regularization penalty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mregularization_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularization_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularization_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_function' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate loss from output of activation2 so softmax activation\n",
    "data_loss = loss_function.forward(activation2.output, y)\n",
    "# Calculate regularization penalty\n",
    "regularization_loss = loss_function.regularization_loss(dense1) + \\\n",
    "loss_function.regularization_loss(dense2)\n",
    "# Calculate overall loss\n",
    "loss = data_loss + regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
